"""
Inspired by https://github.com/lucidrains/triton-transformer/blob/main/triton_transformer/softmax.py
"""
import cupy as cp
import triton
import triton.language as tl
from .utils import calc_num_warps

def naive_softmax(x):
   
    # Subtract max for numerical stability
    row_max = cp.max(x, axis=1, keepdims=True)  # shape (n_rows, 1)
    x_stable = x - row_max

    # Exponentiate
    x_exp = cp.exp(x_stable)

    # Sum along rows
    row_sum = cp.sum(x_exp, axis=1, keepdims=True)

    # Divide
    out = x_exp / row_sum

    return out

@triton.heuristics({"num_warps": lambda args: calc_num_warps(args["BLOCK_SIZE"])})
@triton.jit
def softmax_kernel_forward(
    output_ptr, 
    input_ptr, 
    input_row_stride, 
    output_row_stride, 
    dtype_flag: tl.constexpr, # Flag for if our data is float32 or float16
    n_cols: tl.constexpr,  
    BLOCK_SIZE: tl.constexpr):

    """

    ### NAIVE SOFTMAX ###
    In normal softmax, we do the following ops:

    1) max on every row
    2) subtract every row by max
    3) Exponentiate every value
    4) Sum together everything
    5) divide every value by this sum

    That is 5 kernel launches on our GPUs to do this one Op!!!

    ### FUSED SOFTMAX ###
    We will fuse these ops into a single Kernel Launch using Triton!! 

    This is dumb softmax implementation. The idea is that we have the following matrix (N x M), 
    and we want to compute softmax for each N across the M. 

    row 0: [x00 x01 x02 ... x0M]
    row 1: [x10 x11 x12 ... x1M]
    row 2: [x20 x21 x22 ... x2M]
    ...
    row N: [xR0 xR1 xR2 ... xRM]

    ### GPU Programming Basics ###
    Softmax is a great place to start as its pretty easy to setup. Some terminology:
    
    GPUs are massively parallel processes that run thousands of threads at the same time (each thread doing a tiny bit of work)
    This means we need some structure on these threads so we can map different threads to our operation. 

    # TERMINOLOGY
    Thread -> Smallest unit of execution. They perform a sequence of instructions to the data you point to

    Thread Blocks -> A group of threads that work together on a chunk of data. Thread blocks can also 
                        share memory so one thread can refer to data on another thread as long as they are in 
                        that same block. These can have upto 3 dimensions (x,y,z) for complex mappings


    Grid -> The grid is the collection of thread blocks that cover your full tensor. 

    Kernel Launch -> A kernel is a function that runs on GPU. It operates in parallel with many threads executing
                     the same instruction on different chunks of the data. 

    Warp -> Threads are grouped into groups of 32 (one warp) and exectute the instruction in lock-step. 
            Thus a single blocks threads are grouped into warps. 32 is an upper limit though. The larger
            your block the more warps you need. Warps are also executed concurrently, but again has hardwarde
            limitations. Most modern gpus can launch 64 warps (with 32 threads per warp) in parallel

    # TUNING

    Things lie your warp size, block size, etc.. need to be tuned! It is hardware and operation specific. Triton supports
    autotuning, but for now we will just use a dumb heuristic, mainly because our block size is fixed so all we can really
    mess with is the warp

    ```
    def calc_num_warps(block_size):
        num_warps = 4
        if block_size >= 2048:
            num_warps = 8
        if block_size >= 4096:
            num_warps = 16
        return num_warps
    ```

    As you can see as our block size increases we want more warps to better parallize the ops. 
   
    # Example w/ Softmax
    There will be two assumptions we are making here that makes our softmax naive. 
    
    1) Block_Size = Num Columns. I will set the block size to be at least as large as the number of columns I want to process in a row. 
                    This ensures that the threads in the block can collectively cover the entire row.

        LIMITATION: On modern GPUs, the maximum number of threads per block is 1024, if the number of columns exceed that
                    we cannot assign one thread per column directly. In this case, each thread has to process multiple
                    columns in sequence! Thus we lose some of our parallelism. Luckily, this looping is handled for us 
                    by triton automatically! 

                    The actual thread count per block in the underlying CUDA kernel generated by triton is 
                    controlled by num_warps parameter multiplied by the warp size (default of 32). This means if you
                    have 8 warps, each warp with 32 threads, you will have a total of 256 threads being used. You can 
                    go upto 32 warps for an effective 32 * 32 = 1024 which is the hardware max. 

                    When block size exceeds this thread counts (i.e. BLOCK_SIZE=2048) and we only have 256 threads, Triton
                    will automagically generate code where each thread processes multiple elements sequentially within the block. 


    2) Each block will process one row

        LIMITATION: The expensive part of GPU compute is the CPU/GPU communication overhead. And every kernel launch is expensive. 
                    In our case, we will launch a bunch of tiny kernels, each processing a row. In the example provided in the 
                    Triton tutorials (https://triton-lang.org/main/getting-started/tutorials/02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py)
                    you will see that they actually loop over the rows. This means each kernel launch is processing a chunk of rows
                    at a time rather than one at a time. Basically we want every kernel launch to do more work!


    # Structure of Kernels

    Lets say we have the following input matrix. This has 4 rows and 8 columns

    Input matrix:
    row 0: [x00 x01 x02 x03 x04 x05 x06 x07]
    row 1: [x10 x11 x12 x13 x14 x15 x16 x17]
    row 2: [x20 x21 x22 x23 x24 x25 x26 x27]
    row 3: [x30 x31 x32 x33 x34 x35 x36 x37]

    This means we will define a block size of 8 and a grid size of 4. This is because we have 8 things we want 
    to compute over in every block, and we have 4 rows. We will then get something like this

    Thread Block 0 -> processes row 0
    Thread Block 1 -> processes row 1
    Thread Block 2 -> processes row 2
    Thread Block 3 -> processes row 3

    Now inside a single block, we have our 8 threads doing something

    Block 0 (row 0):
    Thread 0 -> x00
    Thread 1 -> x01
    Thread 2 -> x02
    Thread 3 -> x03
    Thread 4 -> x04
    Thread 5 -> x05
    Thread 6 -> x06
    Thread 7 -> x07

    ### Strides Tell Us the Layout of the Matrix ###

    Now we see matricies as a 2d array in this case. But internally on the GPU, its organized as a single long vector. The stride
    tells us the relation of our matrix this vector. For example we see our matrix as:

    (4 x 8) Input matrix:
    [x00 x01 x02 x03 x04 x05 x06 x07]
    [x10 x11 x12 x13 x14 x15 x16 x17]
    [x20 x21 x22 x23 x24 x25 x26 x27]
    [x30 x31 x32 x33 x34 x35 x36 x37]

    But internally its stored as:

    [x00 x01 x02 x03 x04 x05 x06 x07 x10 x11 x12 x13 x14 x15 x16 x17 x20 x21 x22 x23 x24 x25 x26 x27 x30 x31 x32 x33 x34 x35 x36 x37]

    The stride tells me the number of elements I need to jump in memory (this vector form) to move from one row (or column) to the next. 
    In this case, to go from one column to the next, I only need to move over one element! But to move from one row to the next I need
    to move over 8 elements:

    Thus:

        row_stride = 8
        col_stride = 1

    This can be more complicated for higher dimensional arrays, but the principle is the same. For every dimension in your array, how many 
    elements do you need to move to get to the next index of that dimension. 

    ### Pointers ###
    Triton (like CUDA) uses pointers to do their operations. A pointer is just a fancy way of saying, "address to specific location in memory". 
    In most cases, the pointer will point to the STARTING value of your matrix (in our case x00). We then need to use offsets to get to the 
    actual location we want. For example:

    If we input_ptr points to x00, and we want the start of every row:

        x10 = input_ptr + 8 = input_ptr + row_stride * 1
        x20 = input_ptr + 16 = input_ptr + row_stride * 2
        x30 = input_ptr + 24 = input_ptr + row_stride * 3

    ######## SUPER CAVEAT!!! ############
    We will be using CuPY as triton works with anything that has a GPU pointer. torch.stides() returns the number of elements you need to 
    move over to go to the next row. Cupy.stride returns the number of bytes you need to move to go to the next row! This means when working 
    in cupy, we have to convert. For example. 

    ```
    rand_torch = torch.randn((32,32), dtype=torch.float32)
    print(rand_torch.stride()) -> (32,1) 

    rand = cp.random.normal(size=(32,32), dtype=cp.float32)
    print(rand.strides) -> (128,4)
    
    ```

    As we can see, cupy gives (128,4,) because it says you have to move over 128 bytes to get to the next row and 4 bytes to get to the next 
    column. Well in float32 precision, each element has 4 bytes. So if you divide, its (128/4=32, 4/4=1) = (32,1) which is the same as torch!

    In cupy we can get this easily with rand.itemsize

    ```
    rand = cp.random.normal(size=(32,32), dtype=cp.float32)
    print(rand.strides) -> (128,4)
    print(rand.strides[0]/rand.itemsize, rand.strides[1]/rand.itemsize) -> (32,1)
    ```

    ### Saving Results ###
    Once we have done our computation, we need a place to save it. This is why triton kernels take both an input pointer (the data you
    want to perform the operation on) and the output pointer (the matrix you want to store these results in). Softmax has the same input
    and output shape so its easy, but other ops could require a bit more effort in indexing/


    ### TLDR ###
    GPU Programming is all about 2 things:

        1) Mapping indexes of your threads/threadblocks to a grid that covers your whole tensor efficiently. This requires some
           pointer arithmetic to index correctly. 
        2) Tuning block sizes and warps to optimize parallelism and memory pressure 

    """
    
    ### Each thread block will process one row. I provide ahead of time the ###
    ### number of thread blocks (its our grid) which in thise case is just number of rows ###
    ### Thus this thread block index is just our row index ###
    row_idx = tl.program_id(0)

    ### SUPER CAVEAT!!! THIS IS SPECIFIC TO CUPY. THE DTYPE OF CUPY POINTERS ARE INT64 BUT ###
    ### TRITON EXPECTS THEM IN THE TYPE OF THE MATRIX WE ARE WORKING ON ###
    ### SO THIS IS A QUICK TOGGLE TO CAST THE DTYPE ###
    ### CODE REFERENCE: https://github.com/triton-lang/triton/issues/2943

    if dtype_flag == 0:  # float32
        input_ptr = tl.cast(input_ptr, tl.pointer_type(tl.float32))
        output_ptr = tl.cast(output_ptr, tl.pointer_type(tl.float32))
    elif dtype_flag == 1:  # float16
        input_ptr = tl.cast(input_ptr, tl.pointer_type(tl.float16))
        output_ptr = tl.cast(output_ptr, tl.pointer_type(tl.float16))


    ### We have the row index in our threadblock, but we actually need to grab that row of data ###
    ### from memory. So lets index it! Our input_ptr points to the starting point of our data ###
    ### so we just need to index to the row_idxth row of our matrix. 
    row_start_ptr = input_ptr + row_idx * input_row_stride

    ### Now that we have the starting point of a row, we need indexes for the full column ###
    ### for example if we are at index 8 and there are 8 elements we need to do ###
    ### 8 + [0,1,2,3,4,5,6,7] -> [8,9,10,11,12,13,14,15] ###
    col_offsets = tl.arange(0, BLOCK_SIZE)
    input_ptrs = row_start_ptr + col_offsets

    ### Mask invalid positions. Now here is a consideration for Triton. Each block must have ###
    ### a power of two number of elements! This is fine if we were computing softmax over a  ###
    ### power of two number of elements (like 8). But what if we had 7 elements? ###
    ### Lets say for the first row, we have 7 elements, so the indexes would be ###
    ### [0,1,2,3,4,5,6] ###
    ### But our block size has to be 8 as that is the closest power of 2 larger than 7. Then our ###
    ### block size will try to index [0,1,2,3,4,5,6,7]! But this it not correct! If each row had only ###
    ### 7 elements, the block will index the entire row as well as the first element of the next row. ###
    ### thus we have a mask, that tells triton which indexes we have are valid and which are invalid. ###
    ### if our col_offsets [0,1,2,3,4,5,6,7] is less than 7 we are good, otherwise its masked out! ###
    mask = col_offsets < n_cols

    ### Now that we have indexes its time to load our data ###
    ### But another consideration. Invalid values that are indexed, although are masked out and are not read ###
    ### in, triton will still load something, itll just be some undefined value. So we have to provide some ###
    ### fill value that wont affect our operations. This depends on what we are doing. In softmax we have to ###
    ### take a max and exponeniate + sum. So if we fill in -inf, the max stays the same as before, and after ###
    ### exponentiating (whcih makes it 0) it has to effect on the sum ###
    row = tl.load(input_ptrs, mask=mask, other=-float("inf"))

    ### Now that I have my row lets compute softmax like normal! ###
    row_minus_max = row - tl.max(row, axis=0)
    numerator = tl.exp(row_minus_max)
    denominator = tl.sum(numerator, axis=0)
    softmax_output = numerator / denominator

    ### Store output in our output matrix. Again we compute the pointer to the output (which in this case is the) ###
    ### same logic as the input so pretty easy !! ###
    output_row_start_ptr = output_ptr + row_idx * output_row_stride
    output_ptrs = output_row_start_ptr + col_offsets
    tl.store(output_ptrs, softmax_output, mask=mask)

@triton.heuristics({"num_warps": lambda args: calc_num_warps(args["BLOCK_SIZE"])})
@triton.jit
def softmax_kernel_backward(
    output_ptr, output_row_stride, 
    s_ptr, s_row_stride,
    grad_ptr, grad_row_stride,
    dtype_flag: tl.constexpr,
    n_cols: tl.constexpr, 
    BLOCK_SIZE: tl.constexpr):

    """
    The backward pass is identical to our original softmax backward
    Softmax derivative: grad_input = s * (grad - sum(grad*s))

    where s is the output of our softmax operation and grad is our upstream gradient 
    that we are propagating through.

    So what we have to do is load our grad and our s and multiply them together! Remember
    that our softmax operation doesn't change the shape. This means the shape of our matrix
    that when into softmax, the shape our our output of softmax and the shape of the gradients
    are all the same, which makes it very easy for us!

    So what we will do is load a row from both our grad and s, peform the op and then save it in output 
    """

    row_idx = tl.program_id(0)

    if dtype_flag == 0:  # float32
        s_ptr = tl.cast(s_ptr, tl.pointer_type(tl.float32))
        grad_ptr = tl.cast(grad_ptr, tl.pointer_type(tl.float32))
        output_ptr = tl.cast(output_ptr, tl.pointer_type(tl.float32))
    elif dtype_flag == 1:  # float16
        s_ptr = tl.cast(s_ptr, tl.pointer_type(tl.float16))
        grad_ptr = tl.cast(grad_ptr, tl.pointer_type(tl.float16))
        output_ptr = tl.cast(output_ptr, tl.pointer_type(tl.float16))
    
    ### Get Rows ###
    s_row_start_ptr = s_ptr + row_idx * s_row_stride
    grad_row_start_ptr = grad_ptr + row_idx * grad_row_stride

    col_offsets = tl.arange(0, BLOCK_SIZE)
    s_ptrs = s_row_start_ptr + col_offsets
    grad_ptrs = grad_row_start_ptr + col_offsets

    mask = col_offsets < n_cols

    ### Load the Rows. Now beause we are going to multiply and then add things together ###
    ### filling our invalid values with 0 makes sense as multiplying sets to 0 and they ###
    ### no longer have any contribution ###
    s_row = tl.load(s_ptrs, mask=mask, other=0.)
    grad_row = tl.load(grad_ptrs, mask=mask, other=0.)

    ### Perform the computation ###
    # s * (grad - sum(grad*s)) -> s*grad - s*sum(s*grad)
    grad_x_s = grad_row * s_row
    softmax_grad_out = grad_x_s - s_row * tl.sum(grad_x_s, axis=0)

    ### store in output ###
    output_row_start_ptr = output_ptr + row_idx * output_row_stride
    output_ptrs = output_row_start_ptr + col_offsets
    tl.store(output_ptrs, softmax_grad_out, mask=mask)


def fused_softmax_forward(x):

    n_rows, n_cols = x.shape
    y = cp.empty_like(x)

    BLOCK_SIZE = triton.next_power_of_2(n_cols)

    # Compute strides in elements
    row_stride = x.strides[0] // x.itemsize

    # Map dtype to Triton flag
    dtype_flag = 0 if x.dtype == cp.float32 else 1  # 0=float32, 1=float16

    grid = (n_rows,)
    softmax_kernel_forward[grid](
        y.data.ptr,       # output_ptr
        x.data.ptr,       # input_ptr
        row_stride,       # input_row_stride
        row_stride,       # output_row_stride
        dtype_flag,       # dtype_flag (constexpr)
        n_cols,           # n_cols (constexpr)
        BLOCK_SIZE        # BLOCK_SIZE (constexpr)
    )
    return y

def fused_softmax_backward(grad, s):

    n_rows, n_cols = grad.shape
    grad_input = cp.empty_like(grad)

    BLOCK_SIZE = triton.next_power_of_2(n_cols)

    row_stride = grad.strides[0] // grad.itemsize

    dtype_flag = 0 if grad.dtype == cp.float32 else 1

    grid = (n_rows,)
    softmax_kernel_backward[grid](
        grad_input.data.ptr,  # output_ptr
        row_stride,           # output_row_stride
        s.data.ptr,           # s_ptr
        row_stride,           # s_row_stride
        grad.data.ptr,        # grad_ptr
        row_stride,           # grad_row_stride
        dtype_flag,           # dtype_flag (constexpr)
        n_cols,               # n_cols (constexpr)
        BLOCK_SIZE            # BLOCK_SIZE (constexpr)
    )
    return grad_input

if __name__ == "__main__":

    import torch
    import time
    import matplotlib.pyplot as plt

    ### TORCH KERNELS FOR COMPARISON ###
    # PyTorch wrappers
    def fused_torch_softmax_forward(x):
        n_rows, n_cols = x.shape
        y = torch.empty_like(x)
        BLOCK_SIZE = triton.next_power_of_2(n_cols)
        dtype_flag = 0 if x.dtype == torch.float32 else 1
        grid = (n_rows,)
        softmax_kernel_forward[grid](
            y, x,
            x.shape[1], y.shape[1],
            dtype_flag, n_cols, BLOCK_SIZE
        )
        return y

    def fused_torch_softmax_backward(grad, s):
        n_rows, n_cols = grad.shape
        grad_input = torch.empty_like(grad)
        BLOCK_SIZE = triton.next_power_of_2(n_cols)
        dtype_flag = 0 if grad.dtype == torch.float32 else 1
        grid = (n_rows,)
        softmax_kernel_backward[grid](
            grad_input, grad_input.shape[1],
            s, s.shape[1],
            grad, grad.shape[1],
            dtype_flag, n_cols, BLOCK_SIZE
        )
        return grad_input
        
    def test_forward(N, M, n_trials=500):
        print(f"=== N={N}, M={M} ===")

        x_torch = torch.randn(N, M, device='cuda', dtype=torch.float32)
        x_cupy = cp.array(x_torch.detach().cpu().numpy(), dtype=cp.float32)

        # FLOPs estimate
        flops = 5 * N * M

        ### PyTorch softmax ###
        torch.cuda.synchronize()
        t0 = time.time()
        for _ in range(n_trials):
            y_pt = torch.softmax(x_torch, dim=-1)
        torch.cuda.synchronize()
        t_pt = (time.time() - t0) / n_trials
        flops_pt = flops / t_pt / 1e9  # GFLOPS

        ### Naive CuPy ###
        cp.cuda.runtime.deviceSynchronize()
        t0 = time.time()
        for _ in range(n_trials):
            y_naive = naive_softmax(x_cupy)
        cp.cuda.runtime.deviceSynchronize()
        t_naive = (time.time() - t0) / n_trials
        flops_naive = flops / t_naive / 1e9


        ### Triton LayerNorm on CuPy ###
        # Warmup: run once to JIT compile kernel and load it on GPU
        _ = fused_softmax_forward(x_cupy)
        cp.cuda.runtime.deviceSynchronize()

        t0 = time.time()
        for _ in range(n_trials):
            y_triton_cupy = fused_softmax_forward(x_cupy)
        cp.cuda.runtime.deviceSynchronize()
        t_triton_cupy = (time.time() - t0) / n_trials
        flops_triton_cupy = flops / t_triton_cupy / 1e9

        # Verify correctness
        max_diff_naive = cp.max(cp.abs(y_naive - cp.array(torch.softmax(x_torch, dim=-1).cpu().numpy()))).item()
        max_diff_triton_cupy = cp.max(cp.abs(y_triton_cupy - cp.array(torch.softmax(x_torch, dim=-1).cpu().numpy()))).item()

        print(f"PyTorch softmax:   {t_pt*1000:.2f} ms, {flops_pt:.2f} GFLOP/s")
        print(f"Naive CuPy:        {t_naive*1000:.2f} ms, {flops_naive:.2f} GFLOP/s, max diff {max_diff_naive:.3e}")
        print(f"Triton CuPy:       {t_triton_cupy*1000:.2f} ms, {flops_triton_cupy:.2f} GFLOP/s, max diff {max_diff_triton_cupy:.3e}")
        print("-"*50)

        return {
            "M": M,
            "pt_time": t_pt*1000, "pt_flops": flops_pt,
            "naive_time": t_naive*1000, "naive_flops": flops_naive,
            "triton_cupy_time": t_triton_cupy*1000, "triton_cupy_flops": flops_triton_cupy,
        }

    sizes = torch.arange(128, 16385, 128)
    sizes = [(4096, 128), (4096, 256), (4096, 512), (4096, 1024), (4096, 2048), (4096, 4096),(4096, 8192)]

    results = []
    for N, M in sizes:
        results.append(test_forward(N, M, n_trials=10))

    # Convert to plotting arrays
    Ms = [r["M"] for r in results][1:]

    plt.figure(figsize=(10,5))
    # --- Time plot ---
    plt.subplot(1,2,1)
    plt.plot(Ms, [r["pt_time"] for r in results][1:], "-o", label="Pytorch", linewidth=5)
    plt.plot(Ms, [r["naive_time"] for r in results][1:], "-o", label="Naive CuPy", linewidth=5)
    plt.plot(Ms, [r["triton_cupy_time"] for r in results][1:], "-o", label="Triton CuPy", linewidth=5)
    plt.xlabel("Sequence length (M)")
    plt.ylabel("Time (ms)")
    plt.title("Softmax Runtime vs Sequence Length")
    plt.legend()
    plt.grid(True)

    # --- FLOPs plot ---
    plt.subplot(1,2,2)
    plt.plot(Ms, [r["pt_flops"] for r in results][1:], "-o", label="Pytorch", linewidth=5)
    plt.plot(Ms, [r["naive_flops"] for r in results][1:], "-o", label="Naive CuPy", linewidth=5)
    plt.plot(Ms, [r["triton_cupy_flops"] for r in results][1:], "-o", label="Triton CuPy", linewidth=5)
    plt.xlabel("Sequence length (M)")
    plt.ylabel("Throughput (GFLOP/s)")
    plt.title("Softmax FLOPs vs Sequence Length")
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig("benchmark/softmax.png")
    plt.show()

    def test_backward(N=1024, M=512):
        print(f"=== Backward Test: N={N}, M={M} ===")

        # --------------------------
        # Torch setup
        # --------------------------
        x_torch = torch.randn(N, M, device="cuda", dtype=torch.float32, requires_grad=True)
        y_pt = torch.softmax(x_torch, dim=-1)

        grad_out_torch = torch.randn_like(x_torch)

        # Torch backward
        y_pt.backward(grad_out_torch)
        grad_pt = x_torch.grad.detach()

        # --------------------------
        # CuPy setup
        # --------------------------
        x_cupy = cp.array(x_torch.detach().cpu().numpy(), dtype=cp.float32)
        s_cupy = cp.array(y_pt.detach().cpu().numpy(), dtype=cp.float32)
        grad_out_cupy = cp.array(grad_out_torch.detach().cpu().numpy(), dtype=cp.float32)

        # Triton backward (CuPy)
        grad_triton_cupy = fused_softmax_backward(grad_out_cupy, s_cupy)

        # --------------------------
        # Compare results
        # --------------------------
        grad_triton_torch = torch.from_numpy(cp.asnumpy(grad_triton_cupy)).to("cuda")

        max_diff = (grad_triton_torch - grad_pt).abs().max().item()
        print(f"Max abs diff: {max_diff:.3e}")
        print("-"*50)

        return max_diff


    # Run test for a few sizes
    for N, M in [(4, 8), (128, 256), (1024, 512), (1024, 2048)]:
        test_backward(N, M)